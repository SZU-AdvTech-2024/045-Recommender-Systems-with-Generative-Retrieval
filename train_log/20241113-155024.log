/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/transformers/utils/import_utils.py:455: FutureWarning: The util is_torch_bf16_available is deprecated, please use is_torch_bf16_gpu_available or is_torch_bf16_cpu_available instead according to whether it's used with cpu or gpu
  warnings.warn(
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
2024-11-13 15:50:46,298 - INFO - hyper-param:
2024-11-13 15:50:46,298 - INFO -   output_dir: ./checkpoints
2024-11-13 15:50:46,298 - INFO -   batch_size: 128
2024-11-13 15:50:46,298 - INFO -   bf16: True
2024-11-13 15:50:46,298 - INFO -   epoch: 200
2024-11-13 15:50:46,298 - INFO -   optimizer: adamw_torch
2024-11-13 15:50:46,298 - INFO -   lr: 0.0005
2024-11-13 15:50:46,298 - INFO -   weight_decay: 0.01
2024-11-13 15:50:46,298 - INFO -   lr_scheduler_type: cosine
2024-11-13 15:50:46,298 - INFO -   warmup_ratio: 0.01
2024-11-13 15:50:46,298 - INFO -   gradient_accumulation_steps: 2
2024-11-13 15:50:46,299 - INFO -   plm_dir: ../LLM/
2024-11-13 15:50:46,299 - INFO -   plm_name: t5-base
2024-11-13 15:50:46,299 - INFO -   tokenizer_plm: sentence-t5-base
2024-11-13 15:50:46,299 - INFO -   dataset: Games
2024-11-13 15:50:46,299 - INFO -   token_type: sid_nc
2024-11-13 15:50:46,299 - INFO -   K: 256
2024-11-13 15:50:46,299 - INFO -   D: 3
2024-11-13 15:50:46,299 - INFO -   add_user_prefix: False
2024-11-13 15:50:46,299 - INFO -   user_prefix: <u_{}>
2024-11-13 15:50:46,299 - INFO -   item_sep: ,
2024-11-13 15:50:46,299 - INFO -   max_len: 20
2024-11-13 15:50:46,299 - INFO -   max_sent_len: 512
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
2024-11-13 15:50:46,597 - WARNING - Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 4.9359, 'grad_norm': 0.14638656377792358, 'learning_rate': 0.00021222410865874363, 'epoch': 0.8488964346349746}
{'eval_loss': 3.8077642917633057, 'eval_runtime': 89.3677, 'eval_samples_per_second': 565.596, 'eval_steps_per_second': 2.216, 'epoch': 1.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 3.7692, 'grad_norm': 0.23217909038066864, 'learning_rate': 0.00042444821731748726, 'epoch': 1.697792869269949}
{'eval_loss': 3.3155527114868164, 'eval_runtime': 92.3822, 'eval_samples_per_second': 547.14, 'eval_steps_per_second': 2.143, 'epoch': 2.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 3.3476, 'grad_norm': 0.21642610430717468, 'learning_rate': 0.000499990595021466, 'epoch': 2.5466893039049237}
{'eval_loss': 2.678950071334839, 'eval_runtime': 90.7682, 'eval_samples_per_second': 556.869, 'eval_steps_per_second': 2.181, 'epoch': 3.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 2.8911, 'grad_norm': 0.3213983178138733, 'learning_rate': 0.0004999387121088622, 'epoch': 3.395585738539898}
{'eval_loss': 2.372953414916992, 'eval_runtime': 90.0913, 'eval_samples_per_second': 561.053, 'eval_steps_per_second': 2.198, 'epoch': 4.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 2.6071, 'grad_norm': 0.27119070291519165, 'learning_rate': 0.0004998414866665849, 'epoch': 4.244482173174872}
{'eval_loss': 2.212350368499756, 'eval_runtime': 92.6864, 'eval_samples_per_second': 545.344, 'eval_steps_per_second': 2.136, 'epoch': 5.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 2.4335, 'grad_norm': 0.3608861565589905, 'learning_rate': 0.0004996989363327479, 'epoch': 5.093378607809847}
{'loss': 2.3137, 'grad_norm': 2.6827845573425293, 'learning_rate': 0.000499511086968062, 'epoch': 5.942275042444821}
{'eval_loss': 2.1030473709106445, 'eval_runtime': 86.0712, 'eval_samples_per_second': 587.258, 'eval_steps_per_second': 2.3, 'epoch': 6.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 2.2177, 'grad_norm': 0.3362637162208557, 'learning_rate': 0.0004992779726511433, 'epoch': 6.791171477079796}
{'eval_loss': 2.0077011585235596, 'eval_runtime': 99.0813, 'eval_samples_per_second': 510.147, 'eval_steps_per_second': 1.998, 'epoch': 7.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 2.1401, 'grad_norm': 0.2519213557243347, 'learning_rate': 0.0004989996356723307, 'epoch': 7.6400679117147705}
{'eval_loss': 1.9352028369903564, 'eval_runtime': 102.3769, 'eval_samples_per_second': 493.725, 'eval_steps_per_second': 1.934, 'epoch': 8.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 2.0652, 'grad_norm': 0.30964869260787964, 'learning_rate': 0.0004986761265260137, 'epoch': 8.488964346349745}
{'eval_loss': 1.8699766397476196, 'eval_runtime': 100.7536, 'eval_samples_per_second': 501.679, 'eval_steps_per_second': 1.965, 'epoch': 9.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 2.0144, 'grad_norm': 0.24284674227237701, 'learning_rate': 0.0004983075039014725, 'epoch': 9.33786078098472}
{'eval_loss': 1.8316869735717773, 'eval_runtime': 104.2105, 'eval_samples_per_second': 485.037, 'eval_steps_per_second': 1.9, 'epoch': 10.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 1.9633, 'grad_norm': 0.32116222381591797, 'learning_rate': 0.0004978938346722302, 'epoch': 10.186757215619695}
{'eval_loss': 1.7979124784469604, 'eval_runtime': 99.592, 'eval_samples_per_second': 507.531, 'eval_steps_per_second': 1.988, 'epoch': 11.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 1.9279, 'grad_norm': 0.3876384198665619, 'learning_rate': 0.0004974351938839219, 'epoch': 11.03565365025467}
{'loss': 1.8916, 'grad_norm': 0.31822386384010315, 'learning_rate': 0.0004969316647406791, 'epoch': 11.884550084889643}
{'eval_loss': 1.7625960111618042, 'eval_runtime': 100.476, 'eval_samples_per_second': 503.066, 'eval_steps_per_second': 1.971, 'epoch': 12.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 1.8619, 'grad_norm': 0.22029580175876617, 'learning_rate': 0.0004963833385900365, 'epoch': 12.733446519524618}
{'eval_loss': 1.7453875541687012, 'eval_runtime': 93.9935, 'eval_samples_per_second': 537.761, 'eval_steps_per_second': 2.107, 'epoch': 13.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 1.8369, 'grad_norm': 0.3352351784706116, 'learning_rate': 0.0004957903149063593, 'epoch': 13.582342954159593}
{'eval_loss': 1.7217907905578613, 'eval_runtime': 94.5798, 'eval_samples_per_second': 534.427, 'eval_steps_per_second': 2.093, 'epoch': 14.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 1.8156, 'grad_norm': 0.22233392298221588, 'learning_rate': 0.0004951527012727975, 'epoch': 14.431239388794568}
{'eval_loss': 1.7116093635559082, 'eval_runtime': 94.0015, 'eval_samples_per_second': 537.715, 'eval_steps_per_second': 2.106, 'epoch': 15.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 1.7945, 'grad_norm': 0.2559391260147095, 'learning_rate': 0.0004944706133617689, 'epoch': 15.280135823429541}
{'eval_loss': 1.6898279190063477, 'eval_runtime': 95.9273, 'eval_samples_per_second': 526.92, 'eval_steps_per_second': 2.064, 'epoch': 16.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 1.7772, 'grad_norm': 0.2045511156320572, 'learning_rate': 0.0004937441749139742, 'epoch': 16.129032258064516}
{'loss': 1.7611, 'grad_norm': 0.20624245703220367, 'learning_rate': 0.0004929735177159484, 'epoch': 16.97792869269949}
{'eval_loss': 1.68009352684021, 'eval_runtime': 100.5144, 'eval_samples_per_second': 502.873, 'eval_steps_per_second': 1.97, 'epoch': 17.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 1.741, 'grad_norm': 0.23053401708602905, 'learning_rate': 0.0004921587815761533, 'epoch': 17.826825127334466}
{'eval_loss': 1.6670100688934326, 'eval_runtime': 97.5276, 'eval_samples_per_second': 518.274, 'eval_steps_per_second': 2.03, 'epoch': 18.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 1.7256, 'grad_norm': 0.20631691813468933, 'learning_rate': 0.0004913001142996139, 'epoch': 18.67572156196944}
{'eval_loss': 1.6573408842086792, 'eval_runtime': 101.9932, 'eval_samples_per_second': 495.582, 'eval_steps_per_second': 1.941, 'epoch': 19.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 1.7124, 'grad_norm': 0.36145272850990295, 'learning_rate': 0.0004903976716611045, 'epoch': 19.524617996604412}
{'eval_loss': 1.646407961845398, 'eval_runtime': 99.8877, 'eval_samples_per_second': 506.028, 'eval_steps_per_second': 1.982, 'epoch': 20.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 1.6943, 'grad_norm': 0.30258381366729736, 'learning_rate': 0.0004894516173768889, 'epoch': 20.37351443123939}
{'eval_loss': 1.6373395919799805, 'eval_runtime': 101.4408, 'eval_samples_per_second': 498.281, 'eval_steps_per_second': 1.952, 'epoch': 21.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 1.682, 'grad_norm': 0.42426812648773193, 'learning_rate': 0.0004884621230750198, 'epoch': 21.222410865874362}
{'eval_loss': 1.6206352710723877, 'eval_runtime': 97.4048, 'eval_samples_per_second': 518.927, 'eval_steps_per_second': 2.033, 'epoch': 22.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 1.6695, 'grad_norm': 0.23689821362495422, 'learning_rate': 0.00048742936826420293, 'epoch': 22.07130730050934}
{'loss': 1.654, 'grad_norm': 0.670486330986023, 'learning_rate': 0.00048635354030123184, 'epoch': 22.920203735144312}
{'eval_loss': 1.6111923456192017, 'eval_runtime': 95.0345, 'eval_samples_per_second': 531.87, 'eval_steps_per_second': 2.083, 'epoch': 23.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 1.6411, 'grad_norm': 0.24087630212306976, 'learning_rate': 0.00048523483435699823, 'epoch': 23.769100169779286}
{'eval_loss': 1.609328269958496, 'eval_runtime': 99.0717, 'eval_samples_per_second': 510.196, 'eval_steps_per_second': 1.999, 'epoch': 24.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 1.6268, 'grad_norm': 0.432504802942276, 'learning_rate': 0.00048407345338108545, 'epoch': 24.617996604414262}
{'eval_loss': 1.5982860326766968, 'eval_runtime': 95.754, 'eval_samples_per_second': 527.874, 'eval_steps_per_second': 2.068, 'epoch': 25.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 1.6167, 'grad_norm': 0.19218666851520538, 'learning_rate': 0.0004828696080649503, 'epoch': 25.466893039049236}
{'eval_loss': 1.5927139520645142, 'eval_runtime': 92.7006, 'eval_samples_per_second': 545.261, 'eval_steps_per_second': 2.136, 'epoch': 26.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 1.6019, 'grad_norm': 0.2475750595331192, 'learning_rate': 0.0004816235168037004, 'epoch': 26.31578947368421}
{'eval_loss': 1.584802508354187, 'eval_runtime': 92.1676, 'eval_samples_per_second': 548.414, 'eval_steps_per_second': 2.148, 'epoch': 27.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 1.5926, 'grad_norm': 0.2640271782875061, 'learning_rate': 0.0004803354056564747, 'epoch': 27.164685908319186}
{'eval_loss': 1.5882441997528076, 'eval_runtime': 102.9633, 'eval_samples_per_second': 490.913, 'eval_steps_per_second': 1.923, 'epoch': 28.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 1.5824, 'grad_norm': 0.24358923733234406, 'learning_rate': 0.0004790055083054323, 'epoch': 28.01358234295416}
{'loss': 1.5682, 'grad_norm': 0.29838529229164124, 'learning_rate': 0.00047763406601335955, 'epoch': 28.862478777589136}
{'eval_loss': 1.581588864326477, 'eval_runtime': 100.9776, 'eval_samples_per_second': 500.566, 'eval_steps_per_second': 1.961, 'epoch': 29.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 1.5566, 'grad_norm': 0.214363694190979, 'learning_rate': 0.0004762213275799014, 'epoch': 29.71137521222411}
{'eval_loss': 1.5767117738723755, 'eval_runtime': 98.8318, 'eval_samples_per_second': 511.435, 'eval_steps_per_second': 2.003, 'epoch': 30.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 1.5461, 'grad_norm': 0.24796633422374725, 'learning_rate': 0.00047476754929642536, 'epoch': 30.560271646859082}
{'eval_loss': 1.5779308080673218, 'eval_runtime': 99.2013, 'eval_samples_per_second': 509.53, 'eval_steps_per_second': 1.996, 'epoch': 31.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 1.5362, 'grad_norm': 0.24062064290046692, 'learning_rate': 0.0004732729948995268, 'epoch': 31.40916808149406}
{'eval_loss': 1.574754238128662, 'eval_runtime': 98.1947, 'eval_samples_per_second': 514.753, 'eval_steps_per_second': 2.016, 'epoch': 32.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 1.5282, 'grad_norm': 0.2781561017036438, 'learning_rate': 0.00047173793552318297, 'epoch': 32.25806451612903}
{'eval_loss': 1.576128602027893, 'eval_runtime': 95.5195, 'eval_samples_per_second': 529.169, 'eval_steps_per_second': 2.073, 'epoch': 33.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 1.5179, 'grad_norm': 0.299593061208725, 'learning_rate': 0.00047016264964956547, 'epoch': 33.106960950764005}
{'loss': 1.508, 'grad_norm': 0.36564353108406067, 'learning_rate': 0.0004685474230585195, 'epoch': 33.95585738539898}
{'eval_loss': 1.5797549486160278, 'eval_runtime': 90.7376, 'eval_samples_per_second': 557.057, 'eval_steps_per_second': 2.182, 'epoch': 34.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 1.4943, 'grad_norm': 0.2422298789024353, 'learning_rate': 0.0004668925487757194, 'epoch': 34.80475382003396}
{'eval_loss': 1.5766527652740479, 'eval_runtime': 92.16, 'eval_samples_per_second': 548.459, 'eval_steps_per_second': 2.148, 'epoch': 35.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 1.4842, 'grad_norm': 0.25962033867836, 'learning_rate': 0.00046519832701950924, 'epoch': 35.65365025466893}
{'eval_loss': 1.5871988534927368, 'eval_runtime': 98.5684, 'eval_samples_per_second': 512.801, 'eval_steps_per_second': 2.009, 'epoch': 36.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 1.4743, 'grad_norm': 0.3054846525192261, 'learning_rate': 0.00046346506514643906, 'epoch': 36.502546689303905}
{'eval_loss': 1.5858573913574219, 'eval_runtime': 98.3804, 'eval_samples_per_second': 513.781, 'eval_steps_per_second': 2.013, 'epoch': 37.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 1.4643, 'grad_norm': 0.31347590684890747, 'learning_rate': 0.0004616930775955055, 'epoch': 37.35144312393888}
{'eval_loss': 1.5916186571121216, 'eval_runtime': 95.9774, 'eval_samples_per_second': 526.645, 'eval_steps_per_second': 2.063, 'epoch': 38.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 1.455, 'grad_norm': 0.2694043815135956, 'learning_rate': 0.0004598826858311087, 'epoch': 38.20033955857385}
{'eval_loss': 1.5957468748092651, 'eval_runtime': 95.9307, 'eval_samples_per_second': 526.901, 'eval_steps_per_second': 2.064, 'epoch': 39.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 1.4457, 'grad_norm': 0.39224153757095337, 'learning_rate': 0.00045803421828473324, 'epoch': 39.04923599320883}
{'loss': 1.4318, 'grad_norm': 0.26305097341537476, 'learning_rate': 0.00045614801029536636, 'epoch': 39.898132427843805}
{'eval_loss': 1.5941799879074097, 'eval_runtime': 96.1924, 'eval_samples_per_second': 525.468, 'eval_steps_per_second': 2.058, 'epoch': 40.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 1.4175, 'grad_norm': 0.29756221175193787, 'learning_rate': 0.00045422440404866224, 'epoch': 40.74702886247878}
{'eval_loss': 1.5998197793960571, 'eval_runtime': 90.9572, 'eval_samples_per_second': 555.712, 'eval_steps_per_second': 2.177, 'epoch': 41.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 1.4081, 'grad_norm': 0.29572850465774536, 'learning_rate': 0.0004522637485148648, 'epoch': 41.59592529711375}
{'eval_loss': 1.6078553199768066, 'eval_runtime': 90.464, 'eval_samples_per_second': 558.741, 'eval_steps_per_second': 2.189, 'epoch': 42.0}
There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].
{'train_runtime': 61150.6498, 'train_samples_per_second': 985.602, 'train_steps_per_second': 1.926, 'train_loss': 1.888081783369004, 'epoch': 42.0}
