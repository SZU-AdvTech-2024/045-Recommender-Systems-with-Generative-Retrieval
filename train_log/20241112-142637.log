/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/transformers/utils/import_utils.py:455: FutureWarning: The util is_torch_bf16_available is deprecated, please use is_torch_bf16_gpu_available or is_torch_bf16_cpu_available instead according to whether it's used with cpu or gpu
  warnings.warn(
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
2024-11-12 14:27:01,916 - INFO - hyper-param:
2024-11-12 14:27:01,917 - INFO -   output_dir: ./checkpoints
2024-11-12 14:27:01,917 - INFO -   batch_size: 128
2024-11-12 14:27:01,917 - INFO -   bf16: True
2024-11-12 14:27:01,917 - INFO -   epoch: 200
2024-11-12 14:27:01,917 - INFO -   optimizer: adamw_torch
2024-11-12 14:27:01,917 - INFO -   lr: 0.0005
2024-11-12 14:27:01,917 - INFO -   weight_decay: 0.01
2024-11-12 14:27:01,917 - INFO -   lr_scheduler_type: cosine
2024-11-12 14:27:01,917 - INFO -   warmup_ratio: 0.01
2024-11-12 14:27:01,917 - INFO -   gradient_accumulation_steps: 2
2024-11-12 14:27:01,917 - INFO -   plm_dir: ../LLM/
2024-11-12 14:27:01,917 - INFO -   plm_name: t5-base
2024-11-12 14:27:01,917 - INFO -   tokenizer_plm: sentence-t5-base
2024-11-12 14:27:01,917 - INFO -   dataset: Games
2024-11-12 14:27:01,918 - INFO -   token_type: pretrained
2024-11-12 14:27:01,918 - INFO -   K: 256
2024-11-12 14:27:01,918 - INFO -   D: 3
2024-11-12 14:27:01,918 - INFO -   add_user_prefix: False
2024-11-12 14:27:01,918 - INFO -   user_prefix: <u_{}>
2024-11-12 14:27:01,918 - INFO -   item_sep: ,
2024-11-12 14:27:01,918 - INFO -   max_len: 20
2024-11-12 14:27:01,918 - INFO -   max_sent_len: 512
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
2024-11-12 14:27:02,264 - WARNING - Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'eval_loss': 4.585484504699707, 'eval_runtime': 76.9112, 'eval_samples_per_second': 657.199, 'eval_steps_per_second': 1.716, 'epoch': 0.9987261146496815}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 5.3908, 'grad_norm': 0.1253827065229416, 'learning_rate': 0.00031887755102040814, 'epoch': 1.2738853503184713}
{'eval_loss': 3.984060525894165, 'eval_runtime': 74.5688, 'eval_samples_per_second': 677.844, 'eval_steps_per_second': 1.77, 'epoch': 2.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 4.1651, 'grad_norm': 0.4952516555786133, 'learning_rate': 0.0004999904454116691, 'epoch': 2.5477707006369426}
{'eval_loss': 3.290207624435425, 'eval_runtime': 76.4504, 'eval_samples_per_second': 661.16, 'eval_steps_per_second': 1.727, 'epoch': 2.9987261146496813}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 3.4241, 'grad_norm': 0.26970651745796204, 'learning_rate': 0.0004998950208887939, 'epoch': 3.821656050955414}
{'eval_loss': 2.9016811847686768, 'eval_runtime': 76.7553, 'eval_samples_per_second': 658.534, 'eval_steps_per_second': 1.72, 'epoch': 4.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'eval_loss': 2.6661317348480225, 'eval_runtime': 79.5903, 'eval_samples_per_second': 635.077, 'eval_steps_per_second': 1.658, 'epoch': 4.998726114649681}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 2.9903, 'grad_norm': 0.2567620277404785, 'learning_rate': 0.0004996972482106574, 'epoch': 5.095541401273885}
{'eval_loss': 2.501102924346924, 'eval_runtime': 83.9452, 'eval_samples_per_second': 602.131, 'eval_steps_per_second': 1.572, 'epoch': 6.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 2.7386, 'grad_norm': 0.3338954746723175, 'learning_rate': 0.0004993972083779482, 'epoch': 6.369426751592357}
{'eval_loss': 2.4118242263793945, 'eval_runtime': 74.9664, 'eval_samples_per_second': 674.249, 'eval_steps_per_second': 1.761, 'epoch': 6.998726114649681}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 2.5759, 'grad_norm': 0.2693675756454468, 'learning_rate': 0.0004989950242763614, 'epoch': 7.643312101910828}
{'eval_loss': 2.323629140853882, 'eval_runtime': 79.3914, 'eval_samples_per_second': 636.668, 'eval_steps_per_second': 1.663, 'epoch': 8.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 2.4584, 'grad_norm': 0.3570922613143921, 'learning_rate': 0.0004984908606262696, 'epoch': 8.9171974522293}
{'eval_loss': 2.2627270221710205, 'eval_runtime': 77.1708, 'eval_samples_per_second': 654.989, 'eval_steps_per_second': 1.71, 'epoch': 8.998726114649681}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'eval_loss': 2.2170755863189697, 'eval_runtime': 78.2406, 'eval_samples_per_second': 646.033, 'eval_steps_per_second': 1.687, 'epoch': 10.0}
/home/hcw/miniconda3/envs/pytorch_lqx/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
